2015-04-14 22:27:00+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: JD_spider)
2015-04-14 22:27:00+0800 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-04-14 22:27:00+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'JD_spider.spiders', 'SPIDER_MODULES': ['JD_spider.spiders'], 'LOG_FILE': 'log', 'COOKIES_ENABLED': False, 'BOT_NAME': 'JD_spider'}
2015-04-14 22:27:00+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-14 22:27:01+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-14 22:27:01+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-14 22:27:01+0800 [scrapy] INFO: Enabled item pipelines: JdSpiderPipeline
2015-04-14 22:27:01+0800 [JDspider] INFO: Spider opened
2015-04-14 22:27:01+0800 [JDspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-14 22:27:01+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-04-14 22:27:01+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2015-04-14 22:27:12+0800 [JDspider] DEBUG: Retrying <GET http://club.jd.com/review/10178500-1-4-3.html> (failed 1 times): DNS lookup failed: address 'club.jd.com' not found: [Errno 11002] getaddrinfo failed.
2015-04-14 22:27:14+0800 [JDspider] DEBUG: Crawled (200) <GET http://club.jd.com/review/10178500-1-4-3.html> (referer: None)
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:27:14+0800 [scrapy] NOLEVEL: Append done.
2015-04-14 22:27:14+0800 [JDspider] ERROR: Spider must return Request, BaseItem or None, got 'list' in <GET http://club.jd.com/review/10178500-1-4-3.html>
2015-04-14 22:27:14+0800 [JDspider] ERROR: Spider error processing <GET http://club.jd.com/review/10178500-1-4-3.html>
	Traceback (most recent call last):
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\utils\defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "G:\software\Anaconda\lib\site-packages\scrapy\utils\defer.py", line 96, in iter_errback
	    yield next(it)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "G:\Users\KaNgai\Documents\GitHub\JD_spider\JD_spider\spiders\JDspider.py", line 36, in parse
	    yield Request(url, callback=self.parse)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\http\request\__init__.py", line 26, in __init__
	    self._set_url(url)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\http\request\__init__.py", line 59, in _set_url
	    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
	exceptions.TypeError: Request url must be str or unicode, got list:
	
2015-04-14 22:27:14+0800 [JDspider] INFO: Closing spider (finished)
2015-04-14 22:27:14+0800 [JDspider] INFO: Dumping Scrapy stats:
	{'downloader/exception_count': 1,
	 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
	 'downloader/request_bytes': 472,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 2,
	 'downloader/response_bytes': 13031,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 14, 14, 27, 14, 595000),
	 'log_count/DEBUG': 4,
	 'log_count/ERROR': 2,
	 'log_count/INFO': 28,
	 'response_received_count': 1,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'spider_exceptions/TypeError': 1,
	 'start_time': datetime.datetime(2015, 4, 14, 14, 27, 1, 738000)}
2015-04-14 22:27:14+0800 [JDspider] INFO: Spider closed (finished)
2015-04-14 22:30:24+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: JD_spider)
2015-04-14 22:30:24+0800 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-04-14 22:30:24+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'JD_spider.spiders', 'SPIDER_MODULES': ['JD_spider.spiders'], 'LOG_FILE': 'log', 'COOKIES_ENABLED': False, 'BOT_NAME': 'JD_spider'}
2015-04-14 22:30:24+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-14 22:30:25+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-14 22:30:25+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-14 22:30:25+0800 [scrapy] INFO: Enabled item pipelines: JdSpiderPipeline
2015-04-14 22:30:25+0800 [JDspider] INFO: Spider opened
2015-04-14 22:30:25+0800 [JDspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-14 22:30:25+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-04-14 22:30:25+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2015-04-14 22:30:36+0800 [JDspider] DEBUG: Retrying <GET http://club.jd.com/review/10178500-1-1-0.html> (failed 1 times): DNS lookup failed: address 'club.jd.com' not found: [Errno 11002] getaddrinfo failed.
2015-04-14 22:30:36+0800 [JDspider] DEBUG: Crawled (200) <GET http://club.jd.com/review/10178500-1-1-0.html> (referer: None)
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:30:36+0800 [scrapy] NOLEVEL: Append done.
2015-04-14 22:30:36+0800 [JDspider] ERROR: Spider must return Request, BaseItem or None, got 'list' in <GET http://club.jd.com/review/10178500-1-1-0.html>
2015-04-14 22:30:36+0800 [JDspider] ERROR: Spider error processing <GET http://club.jd.com/review/10178500-1-1-0.html>
	Traceback (most recent call last):
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\utils\defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "G:\software\Anaconda\lib\site-packages\scrapy\utils\defer.py", line 96, in iter_errback
	    yield next(it)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "G:\Users\KaNgai\Documents\GitHub\JD_spider\JD_spider\spiders\JDspider.py", line 36, in parse
	    yield Request(url, callback=self.parse)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\http\request\__init__.py", line 26, in __init__
	    self._set_url(url)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\http\request\__init__.py", line 59, in _set_url
	    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
	exceptions.TypeError: Request url must be str or unicode, got list:
	
2015-04-14 22:30:36+0800 [JDspider] INFO: Closing spider (finished)
2015-04-14 22:30:36+0800 [JDspider] INFO: Dumping Scrapy stats:
	{'downloader/exception_count': 1,
	 'downloader/exception_type_count/twisted.internet.error.DNSLookupError': 1,
	 'downloader/request_bytes': 472,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 2,
	 'downloader/response_bytes': 13748,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 14, 14, 30, 36, 531000),
	 'log_count/DEBUG': 4,
	 'log_count/ERROR': 2,
	 'log_count/INFO': 28,
	 'response_received_count': 1,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'spider_exceptions/TypeError': 1,
	 'start_time': datetime.datetime(2015, 4, 14, 14, 30, 25, 270000)}
2015-04-14 22:30:36+0800 [JDspider] INFO: Spider closed (finished)
2015-04-14 22:34:26+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: JD_spider)
2015-04-14 22:34:26+0800 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-04-14 22:34:26+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'JD_spider.spiders', 'SPIDER_MODULES': ['JD_spider.spiders'], 'LOG_FILE': 'log', 'COOKIES_ENABLED': False, 'BOT_NAME': 'JD_spider'}
2015-04-14 22:34:26+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-14 22:34:28+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-14 22:34:28+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-14 22:34:28+0800 [scrapy] INFO: Enabled item pipelines: JdSpiderPipeline
2015-04-14 22:34:28+0800 [JDspider] INFO: Spider opened
2015-04-14 22:34:28+0800 [JDspider] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-14 22:34:28+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-04-14 22:34:28+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2015-04-14 22:34:36+0800 [JDspider] DEBUG: Crawled (200) <GET http://club.jd.com/review/10178500-1-1-0.html> (referer: None)
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Appending item...
2015-04-14 22:34:36+0800 [scrapy] NOLEVEL: Append done.
2015-04-14 22:34:36+0800 [JDspider] ERROR: Spider must return Request, BaseItem or None, got 'list' in <GET http://club.jd.com/review/10178500-1-1-0.html>
2015-04-14 22:34:36+0800 [JDspider] ERROR: Spider error processing <GET http://club.jd.com/review/10178500-1-1-0.html>
	Traceback (most recent call last):
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "G:\software\Anaconda\lib\site-packages\twisted\internet\task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\utils\defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "G:\software\Anaconda\lib\site-packages\scrapy\utils\defer.py", line 96, in iter_errback
	    yield next(it)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\offsite.py", line 26, in process_spider_output
	    for x in result:
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "G:\software\Anaconda\lib\site-packages\scrapy\contrib\spidermiddleware\depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "G:\Users\KaNgai\Documents\GitHub\JD_spider\JD_spider\spiders\JDspider.py", line 36, in parse
	    yield Request(url, callback=self.parse)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\http\request\__init__.py", line 26, in __init__
	    self._set_url(url)
	  File "G:\software\Anaconda\lib\site-packages\scrapy\http\request\__init__.py", line 59, in _set_url
	    raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)
	exceptions.TypeError: Request url must be str or unicode, got list:
	
2015-04-14 22:34:36+0800 [JDspider] INFO: Closing spider (finished)
2015-04-14 22:34:36+0800 [JDspider] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 236,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 13748,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/200': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 14, 14, 34, 36, 114000),
	 'log_count/DEBUG': 3,
	 'log_count/ERROR': 2,
	 'log_count/INFO': 28,
	 'response_received_count': 1,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 1,
	 'scheduler/enqueued/memory': 1,
	 'spider_exceptions/TypeError': 1,
	 'start_time': datetime.datetime(2015, 4, 14, 14, 34, 28, 164000)}
2015-04-14 22:34:36+0800 [JDspider] INFO: Spider closed (finished)
